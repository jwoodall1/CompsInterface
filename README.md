# Scaffolded Tech Support
*By Katherine Baker, Allie Hopper, Matthew Sigmond, Cuong Tran, and Jonathan Woodall under the supervision of Amy Csizmar Dalal, Carleton College, 2025-2026.*

This project aims to replicate the experiment conducted by Mattias Nückles and Anna Ertelt in their 2006 paper "The problem of describing a problem: Supporting laypersons in presenting their queries to the Internet-based helpdesk." In doing so, we intend to gain a more thorough understanding as to how technical support problem formulation scripts can be optimized in order to increase the quality of experts' problem reconstruction of laypersons' technical issues. Effectively, how can tech support be structured more efficiently, especially for those users who lack technical knowledge? Describing a problem asynchronously can pose significant barriers to mutual understanding of the issue at hand, and a lack of technical expertise can exacerbate the often frustrating nature of these interactions. If tech support help desk users can be given the tools to describe their problem more accurately, we hypothesize that experts will be able to respond to tickets in ways that are more helpful to the person experiencing the problem, in turn increasing the efficiency and effectiveness of help desk interactions. Pending the finished experiment, we plan to submit our research to the Institutional Review Board (IRB) in hopes of publication. 
In replicating the experiment found in Nückles et al. we will give laypersons a set of challenging computer-based tasks to complete, given five minutes per task. If they are unable to finish the task in the given time, they will then write a message to send to a help desk associate that explains the technical problem they are encountering. Laypersons will be randomly assigned to one of two problem formulation scripts: they will write either an entirely unprompted message to the help desk, or a prompted and sequenced (aka, fully scaffolded) message. We will then have our experts read the messages written by the laypersons and attempt to reconstruct what issue the layperson was experiencing. These reconstructions will then be rated by two of us, in order to identify whether the experts could better reconstruct the issues described by scaffolded prompts.
We will identify four sufficiently difficult computer-based tasks to use as material for conducting the experiment as well as identify a pool of up to ninety-six participants who will be split into expert/layperson groups based on a screening questionnaire concerning technical experience with a variety of software. We will use beta testers to determine which tasks we will use in the experiment. We plan to recruit undergraduate Carleton students as participants through outreach to the Carleton ITS Helpdesk student workers, as well as general outreach to the wider student body through email communication. 
We plan to build an interface hosted on a Carleton server wherein participants can complete the tasks assigned to them in the experiment. This interface will allow us to facilitate both data collection and analysis of our findings. After completing the experiment and data analysis, we will present our findings through an academic research paper, a research poster, and a website. 
So, by conducting this research, we plan to answer three questions: (1) Does a prompted "problem formulation script" increase the extensiveness and representativeness of layperson's tech support queries? (2) Can tech experts better reconstruct the problems described by those receiving the sequenced prompts, and if so, can that be attributed to a qualitative or quantitative improvement in the descriptions? (3) Does the structured prompt increase the expert's confidence in their understanding of the problem described by the layperson?